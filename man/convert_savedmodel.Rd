% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/convert.R
\name{convert_savedmodel}
\alias{convert_savedmodel}
\title{Converts a SavedModel}
\usage{
convert_savedmodel(model_dir = NULL, target = "savedmodel.tflite",
  signature_name = "serving_default", inference_type = "FLOAT",
  quantized_input_stats = NULL, drop_control_dependency = TRUE)
}
\arguments{
\item{model_dir}{The path to the exported model, as a string.}

\item{target}{The conversion targer, currently only \code{'.tflite'}
extensions supported to perform TensorFlow lite conversion.}

\item{signature_name}{The named entry point to use in the model for prediction.}

\item{quantized_input_stats}{For each member of input_tensors the mean and
std deviation of training data. Only needed \code{inference_type} is
\code{"QUANTIZED_UINT8"}.}

\item{drop_control_dependency:}{Drops control dependencies silently. This is
due to tf lite not supporting control dependencies.}
}
\description{
Converts a TensorFlow SavedModel into a TensorFlow Lite model optmized
for mobile platforms.
}
